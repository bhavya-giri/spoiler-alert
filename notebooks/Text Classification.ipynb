{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "948c8a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer,Trainer, TrainingArguments,AutoModelForSequenceClassification\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "640220f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28851</td>\n",
       "      <td>Well if you completely enjoy movies with twist...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>284758</td>\n",
       "      <td>What was up with the low cut shirts, anyway? W...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>283169</td>\n",
       "      <td>The Perfect Storm takes a real life event, and...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>428043</td>\n",
       "      <td>I was forced to watch this movie by my girlfri...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>390532</td>\n",
       "      <td>I watched the Dukes of Hazzard as a boy and th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  label\n",
       "0       28851  Well if you completely enjoy movies with twist...      0\n",
       "1      284758  What was up with the low cut shirts, anyway? W...      0\n",
       "2      283169  The Perfect Storm takes a real life event, and...      0\n",
       "3      428043  I was forced to watch this movie by my girlfri...      0\n",
       "4      390532  I watched the Dukes of Hazzard as a boy and th...      0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"../data/train.csv\")\n",
    "valid_df = pd.read_csv(\"../data/valid.csv\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4fbad01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df[\"label\"].value_counts(ascending=True).plot.barh()\n",
    "plt.title(\"Frequency of Classes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43361bb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df[\"Words Per Review\"] = train_df[\"text\"].str.split().apply(len)\n",
    "train_df.boxplot(\"Words Per Review\", by=\"label\", grid=False, showfliers=False,\n",
    "           color=\"gray\")\n",
    "plt.suptitle(\"\")\n",
    "plt.xlabel(\"\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e069bd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.drop(['Words Per Review'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0533c56b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['Unnamed: 0', 'text', 'label'],\n",
       "     num_rows: 45913\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['Unnamed: 0', 'text', 'label'],\n",
       "     num_rows: 11479\n",
       " }))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds = Dataset.from_pandas(train_df)\n",
    "valid_ds = Dataset.from_pandas(valid_df)\n",
    "train_ds,valid_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e53763c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ckpt = \"roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cc7b2d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 8346, 114, 47, 2198, 2254, 4133, 19, 10619, 41677, 6, 42, 34, 300, 10, 372, 10619, 13, 47, 4, 27800, 3904, 6, 89, 18, 460, 402, 2909, 4, 20, 10619, 16, 98, 372, 14, 24, 189, 888, 864, 457, 9, 5, 1569, 47, 95, 3996, 4, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_text = tokenizer(train_df[\"text\"][0])\n",
    "encoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c36c803a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7b1b652",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function tokenize at 0x2a7d6f2e0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cde7a9387ece426499e2ac67068deddc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/45 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f7e3be04df74ad8ba341980e3809160",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_encoded = train_ds.map(tokenize, batched=True, batch_size=1024)\n",
    "valid_encoded = valid_ds.map(tokenize, batched=True, batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "672d6b52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Unnamed: 0', 'text', 'label', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 45913\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad96a51f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "device = torch.device(\"mps\")\n",
    "num_labels = 2\n",
    "model = (AutoModelForSequenceClassification\n",
    "         .from_pretrained(model_ckpt, num_labels=num_labels)\n",
    "         .to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a32dfefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d655107",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9492b19919a84737baee55a59c467f68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f991fa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = \n",
    "logging_steps = len(train_encoded) // batch_size\n",
    "model_name = f\"{model_ckpt}-finetuned-imdb-spoilers\"\n",
    "training_args = TrainingArguments(output_dir=model_name,\n",
    "                                  num_train_epochs=2,\n",
    "                                  learning_rate=2e-5,\n",
    "                                  per_device_train_batch_size=batch_size,\n",
    "                                  per_device_eval_batch_size=batch_size,\n",
    "                                  weight_decay=0.01,\n",
    "                                  evaluation_strategy=\"epoch\",\n",
    "                                  disable_tqdm=False,\n",
    "                                  logging_steps=logging_steps,\n",
    "                                  push_to_hub=True, \n",
    "                                  log_level=\"error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "06ca25cc",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 17.25 GB, other allocations: 878.58 MB, max allowed: 18.13 GB). Tried to allocate 192.00 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(model\u001b[38;5;241m=\u001b[39mmodel, args\u001b[38;5;241m=\u001b[39mtraining_args, \n\u001b[1;32m      2\u001b[0m                   compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[1;32m      3\u001b[0m                   train_dataset\u001b[38;5;241m=\u001b[39mtrain_encoded,\n\u001b[1;32m      4\u001b[0m                   eval_dataset\u001b[38;5;241m=\u001b[39mvalid_encoded,\n\u001b[1;32m      5\u001b[0m                   tokenizer\u001b[38;5;241m=\u001b[39mtokenizer)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m;\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/transformers/trainer.py:1546\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     \u001b[38;5;66;03m# Disable progress bars when uploading models during checkpoints to avoid polluting stdout\u001b[39;00m\n\u001b[1;32m   1545\u001b[0m     hf_hub_utils\u001b[38;5;241m.\u001b[39mdisable_progress_bars()\n\u001b[0;32m-> 1546\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1547\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1548\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1549\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1550\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1551\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1553\u001b[0m     hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/transformers/trainer.py:1837\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1834\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   1836\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1837\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1839\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1840\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1841\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1842\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1843\u001b[0m ):\n\u001b[1;32m   1844\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1845\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/transformers/trainer.py:2682\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2679\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   2681\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2682\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2684\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   2685\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/transformers/trainer.py:2707\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2705\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2706\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2707\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2708\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   2709\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   2710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:1196\u001b[0m, in \u001b[0;36mRobertaForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1189\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1193\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1194\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1196\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroberta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1197\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1202\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1203\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1204\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1205\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1206\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1207\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1208\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(sequence_output)\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:844\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    835\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    837\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m    838\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m    839\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    842\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m    843\u001b[0m )\n\u001b[0;32m--> 844\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    850\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    856\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    857\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:529\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    520\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    521\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    522\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    526\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    527\u001b[0m     )\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 529\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    536\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    539\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    540\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:413\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    403\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    410\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    412\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 413\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    420\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    422\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:340\u001b[0m, in \u001b[0;36mRobertaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    332\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    338\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    339\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m--> 340\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    349\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[1;32m    350\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:270\u001b[0m, in \u001b[0;36mRobertaSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    266\u001b[0m attention_probs \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(attention_scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    268\u001b[0m \u001b[38;5;66;03m# This is actually dropping out entire tokens to attend to, which might\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;66;03m# seem a bit unusual, but is taken from the original Transformer paper.\u001b[39;00m\n\u001b[0;32m--> 270\u001b[0m attention_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_probs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;66;03m# Mask heads if we want to\u001b[39;00m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/torch/nn/modules/dropout.py:59\u001b[0m, in \u001b[0;36mDropout.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/torch/nn/functional.py:1252\u001b[0m, in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1250\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[1;32m   1251\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(p))\n\u001b[0;32m-> 1252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: MPS backend out of memory (MPS allocated: 17.25 GB, other allocations: 878.58 MB, max allowed: 18.13 GB). Tried to allocate 192.00 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "trainer = Trainer(model=model, args=training_args, \n",
    "                  compute_metrics=compute_metrics,\n",
    "                  train_dataset=train_encoded,\n",
    "                  eval_dataset=valid_encoded,\n",
    "                  tokenizer=tokenizer)\n",
    "trainer.train();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7fd7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_output = trainer.predict(valid_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f058a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_output.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ed5169",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_preds, y_true, labels):\n",
    "    cm = confusion_matrix(y_true, y_preds, normalize=\"true\")\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "    disp.plot(cmap=\"Blues\", values_format=\".2f\", ax=ax, colorbar=False)\n",
    "    plt.title(\"Normalized confusion matrix\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85cd94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = np.argmax(preds_output.predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6e3aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [True,False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974d68f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_preds, y_valid, labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
